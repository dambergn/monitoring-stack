// config.alloy

logging {
	level  = "debug" // Set to "info" for less verbose logs once stable
	format = "logfmt"
}

// =================================================================
// METRICS CONFIGURATION
// =================================================================

// Alloy exposes its internal metrics on port 8080 by default.
prometheus.scrape "alloy_internal_metrics" {
  targets    = [{__address__ = "127.0.0.1:8080", __metrics_path__ = "/metrics"}]
  forward_to = [prometheus.remote_write.prometheus_prom_write.receiver]
}

// Configure a remote_write component to send metrics to Prometheus.
prometheus.remote_write "prometheus_prom_write" {
	endpoint {
		url = "http://prometheus:9090/api/v1/write"
	}
}

// Scrape Node Exporter for system metrics
prometheus.scrape "node_exporter_metrics" {
  targets    = [{__address__ = "node-exporter:9100"}]
  forward_to = [prometheus.remote_write.prometheus_prom_write.receiver]
}


// =================================================================
// LOGS CONFIGURATION
// =================================================================

// Define the final destination for ALL logs.
// Using "default" as a label is a common and safe convention.
loki.write "default" {
	endpoint {
		url = "http://loki:3200/loki/api/v1/push"
	}
}


// --- PIPELINE 1: HOST FILE LOGS (Your existing setup) ---

// Discover log files on the host system.
local.file_match "host_logs" {
     path_targets = [
        // Your original pattern for all files ending in .log
        {"__path__" = "/var/log/*.log"},
        
        // ADD THIS: A pattern for a specific file
        {"__path__" = "/var/log/syslog"},
        
        // ADD THIS: Another specific file example
        {"__path__" = "/var/log/dpkg.log"},
        
        // You could also add another wildcard pattern if needed,
        // for example, to get logs from a subdirectory:
        // {"__path__" = "/var/log/nginx/*.log"},
     ]
 }

// Scrape the discovered log files.
loki.source.file "from_host" {
  targets       = local.file_match.host_logs.targets
  forward_to    = [loki.process.filter.receiver] // Forward to the processing block
  tail_from_end = true
}

// Process the logs before they are sent to Loki.
loki.process "filter" {
  // CORRECTED SYNTAX: The 'stage.drop' block is now valid.
  // We removed the unnecessary 'source' attribute.
  stage.drop {
      expression          = ".*Connection closed by authenticating user root"
      drop_counter_reason = "noisy_root_connection"
  }

  // After processing, forward the logs to the final destination.
  forward_to = [loki.write.default.receiver]
}

// --- PIPELINE 2: DOCKER CONTAINER LOGS ---

// STEP 2.1: Discover all running Docker containers.
discovery.docker "all_containers" {
  host = "unix:///var/run/docker.sock"
}

// STEP 2.2: Scrape logs from the discovered containers.
loki.source.docker "from_docker" {
  // THIS IS THE FIX: The 'host' attribute is required here as well.
  host       = "unix:///var/run/docker.sock" 
  
  targets    = discovery.docker.all_containers.targets
  forward_to = [loki.relabel.docker_logs.receiver]
}

// STEP 2.3: Relabel the Docker logs to make them useful.
loki.relabel "docker_logs" {
  forward_to = [loki.write.default.receiver]

  rule {
    replacement  = "integrations/docker"
    target_label = "job"
  }
  rule {
    source_labels = ["__meta_docker_container_name"]
    regex         = "/(.*)"
    target_label  = "container"
  }
}


// =================================================================
// TRACES CONFIGURATION
// =================================================================

// 1. Define an OTLP receiver to accept trace data from applications.
otelcol.receiver.otlp "traces_receiver" {
  grpc {
    endpoint = "0.0.0.0:4319"
  }
  http {
    endpoint = "0.0.0.0:4320"
  }
}

// 2. Define an OTLP exporter to forward traces TO the Tempo service.
otelcol.exporter.otlp "tempo_exporter" {
  client {
    endpoint = "tempo:4317" // Sends to Tempo's gRPC port
    tls {
      insecure = true
    }
  }
}

// 3. Define a processing pipeline.
otelcol.processor.batch "default" {
  output {
    traces = [otelcol.exporter.otlp.tempo_exporter.input]
  }
}

// 4. Activate the pipeline using the CORRECT service component.
otelcol.service.telemetry "default" {
  pipeline "traces" { // Name this pipeline 'traces'
    receivers = [otelcol.receiver.otlp.traces_receiver.output]
    processors = [otelcol.processor.batch.default.input]
  }
}